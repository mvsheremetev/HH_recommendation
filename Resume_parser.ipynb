{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858ac0db",
   "metadata": {},
   "source": [
    "Зададим исходные данные:\n",
    "- количество страниц поисковой выдачи hh.ru для парсинга;\n",
    "- timeout парсера при скачивании страниц поисковой выдачи;\n",
    "- timeout парсера при скачивании страниц резюме.\n",
    "\n",
    "Пояснение: страница поисковой выдачи - страница, генерируемая hh.ru при поисковом запросе на резюме определенных специалистов. Обычно такой запрос выдает 20 ссылок на резюме на каждой странице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba885a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAGES_COUNT = 250\n",
    "HHDownloader_timeout = 5\n",
    "HHResumeDownloader_timeout = 10\n",
    "HHResumeDownloader_count = PAGES_COUNT * 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40dc94",
   "metadata": {},
   "source": [
    "Для скачивания страниц поисковой выдачи объявим класс HHDownloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c57fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class HHDownloader:\n",
    "    def __init__(self, start_url_template: str, data_path: str, timeout=10):\n",
    "        self.start_url_template = start_url_template\n",
    "        self.headers = {'User-Agent':\n",
    "                            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "        self.timeout = timeout\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def check_if_exists(self, page_num: int):\n",
    "#   проверка, если страница уже скачана\n",
    "        return os.path.exists(os.path.join(self.data_path, 'resume_page_{0}.html'.format(page_num)))\n",
    "\n",
    "    def download_pages(self, start_page: int, end_page: int):\n",
    "#   скачивание страниц в заданном диапазоне\n",
    "        for page_num in range(start_page, end_page + 1):\n",
    "            if self.check_if_exists(page_num):\n",
    "                print(f\"File {'resume_page_{0}.html'.format(page_num)} already exists. Skip\")\n",
    "                continue\n",
    "            print(f\"Start downloading {page_num} page\")\n",
    "            page_url = self.get_page_url(page_num)\n",
    "            print(f\"Downloaded {page_num} page\")\n",
    "            page = self.download_page(page_url)\n",
    "            self.save_page(page, page_num)\n",
    "            print(f\"Saved {page_num} page to {os.getcwd()}/data\")\n",
    "            print(\"*\" * 20)\n",
    "            time.sleep(self.timeout)\n",
    "\n",
    "    def get_page_url(self, page_num: int):\n",
    "#   генерация ссылок на страницы\n",
    "        return self.start_url_template.format(page_num)\n",
    "\n",
    "    def download_page(self, url: str):\n",
    "#   скачивание страницы\n",
    "        page = requests.get(url, headers=self.headers)\n",
    "        return page\n",
    "\n",
    "    def save_page(self, page, page_num: int):\n",
    "#   сохранение страниц в текущей директории\n",
    "        page_file_name = os.path.join(self.data_path, 'resume_page_{0}.html'.format(page_num))\n",
    "        with open(page_file_name, 'w') as resume_request:\n",
    "            resume_request.write(page.text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = \"https://hh.ru/search/resume?text=sales+manager&st=resumeSearch&logic=normal&pos=full_text&exp_\" \\\n",
    "                   \"period=all_time&exp_company_size=any&exp_industry=any&area=2&area=1&relocation=living_or_relocati\" \\\n",
    "                   \"on&salary_from=&salary_to=&currency_code=RUR&label=only_with_salary&education=none&age_from=&age_\" \\\n",
    "                   \"to=&gender=unknown&order_by=relevance&search_period=0&items_on_page=20&page={0}\"\n",
    "    hh_downloader = HHDownloader(start_url, \"data/\", timeout=HHDownloader_timeout)\n",
    "    hh_downloader.download_pages(0, PAGES_COUNT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c32a61",
   "metadata": {},
   "source": [
    "Далее сформируем список id резюме для последующей генерации уникальных ссылок на каждое резюме и сохраним его в файле id_lists.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_links(page: str, pattern: str, start: int, end: int):\n",
    "    for i in range(start, end + 1):\n",
    "        with open(page.format(i), 'r') as resume_request, open('data/id_list/id_list.txt', 'a') as id_list:\n",
    "            soup = BeautifulSoup(resume_request, features=\"lxml\")\n",
    "            id_lst = soup.find_all('a', {'class': \"resume-search-item__name\"})\n",
    "            for raw_id in id_lst:\n",
    "                id_list.write(re.search(pattern, raw_id['href']).group(1))\n",
    "                id_list.write('\\n')\n",
    "\n",
    "\n",
    "write_links('data/resume_page_{0}.html', r'resume\\/(.+)\\?', 0, PAGES_COUNT)\n",
    "print('id_lists записан')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97df39a",
   "metadata": {},
   "source": [
    "Скачивание уникальных страниц с резюме кандидатов реализуется через класс HHResumeDownloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0264e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HHResumeDownloader:\n",
    "    def __init__(self, page_url: str, data_path: str, number: int, timeout=10):\n",
    "        self.page_url = page_url\n",
    "        self.headers = {'User-Agent':\n",
    "                            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "        self.timeout = timeout\n",
    "        self.data_path = data_path\n",
    "        self.number = number\n",
    "        self.proxy = {'http': 'http://161.202.226.195:8123'}\n",
    "\n",
    "    def check_if_exists(self, resume_id: str):\n",
    "#   Проверка, была ли уже скачана данная странца\n",
    "        return os.path.exists(os.path.join(self.data_path, f'resume_page_{resume_id}.html'))\n",
    "\n",
    "    def get_page_url(self, resume_id: str):\n",
    "#   Склеиваем ссылку на резюме и уникальный номер страницы\n",
    "        return self.page_url.format(resume_id)\n",
    "\n",
    "    def download_pages(self):\n",
    "#   Скачиваем страницу и сохраняем локально\n",
    "        with open('data/id_list/id_list.txt', 'r') as id_list:\n",
    "            rep = 0\n",
    "            for resume_id in id_list:\n",
    "                resume_id = resume_id.strip()\n",
    "                if self.check_if_exists(resume_id):\n",
    "                    print(f\"File {'resume_page_{0}.html'.format(resume_id)} already exists. Skip\")\n",
    "                    continue\n",
    "                print(f\"Start downloading {resume_id} page\")\n",
    "                page_url = self.get_page_url(resume_id)\n",
    "                print(f\"Downloaded {resume_id} page\")\n",
    "                page = self.download_page(page_url)\n",
    "                self.save_page(page, resume_id)\n",
    "                print(f\"Saved {resume_id} page\")\n",
    "                print(\"*\" * 20)\n",
    "                time.sleep(self.timeout)\n",
    "                rep += 1\n",
    "                if self.number == rep:\n",
    "                    break\n",
    "\n",
    "    def download_page(self, url: str):\n",
    "        page = requests.get(url, headers=self.headers, proxies=self.proxy)\n",
    "        return page\n",
    "\n",
    "    def save_page(self, page, resume_id: str):\n",
    "        page_file_name = os.path.join(self.data_path, 'resume_page_{0}.html'.format(resume_id))\n",
    "        with open(page_file_name, 'w') as resume_request:\n",
    "            resume_request.write(page.text)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hh_resume_downloader = HHResumeDownloader(\"https://hh.ru/resume/{0}\",\n",
    "                                              \"data/saved_resumes/\",\n",
    "                                              HHResumeDownloader_count,\n",
    "                                              timeout=HHResumeDownloader_timeout)\n",
    "    hh_resume_downloader.download_pages()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8257ec",
   "metadata": {},
   "source": [
    "Парсим данные из скачанных html страниц с помощью Beautiful soup и класса Resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bf4920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Resume:\n",
    "#распаковываем информацию из страницы резюме\n",
    "    def __init__(self, soup, resume_id):\n",
    "        self.soup = soup\n",
    "        self.resume_id = resume_id\n",
    "        self.resume_title = self.extract_title()\n",
    "        self.city = self.extract_information('span', {'data-qa': \"resume-personal-address\"})\n",
    "        self.age = self.extract_age()\n",
    "        self.gender = self.extract_gender\n",
    "        self.area = self.extract_area()\n",
    "        self.desired_wage = self.extract_wage()\n",
    "        self.experience_description = self.extract_information('div', {'data-qa': \"resume-block-experience-description\"})\n",
    "        self.work_exp = self.extract_work_experience()\n",
    "        self.education = self.extract_nested_information('div', {'data-qa': \"resume-block-education\"})\n",
    "        self.language_prof = self.extract_languages()\n",
    "        self.skills = self.extract_information('span', {'class': \"bloko-tag__section bloko-tag__section_text\"})\n",
    "        self.dict_resume = self.resume_dict_maker()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'{self.dict_resume}'\n",
    "\n",
    "    def extract_title(self):\n",
    "        title = self.extract_information('span', {'data-qa': \"resume-block-title-position\"})\n",
    "        main_titles = title.split(',')\n",
    "        main_title = main_titles[0]\n",
    "        return main_title\n",
    "\n",
    "    def extract_age(self):\n",
    "        raw_age = self.extract_information('span', {'data-qa': \"resume-personal-age\"})\n",
    "        digit_pattern = r'\\d+'\n",
    "        if raw_age != \"No information\":\n",
    "            age = re.findall(digit_pattern, raw_age)\n",
    "            age = age[0]\n",
    "            return age\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def extract_work_experience(self):\n",
    "        work_exp = self.extract_nested_information('div', {'data-qa': \"resume-block-experience\"})\n",
    "        float_pattern = r'\\d+'\n",
    "        if work_exp != \"No information\":\n",
    "            work_exp = re.findall(float_pattern, work_exp)\n",
    "            if len(work_exp) == 2:\n",
    "                work_exp = float(f'{work_exp[0]}.{work_exp[1]}')\n",
    "            elif len(work_exp) == 1:\n",
    "                work_exp = work_exp[0]\n",
    "            return work_exp\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def extract_area(self):\n",
    "        area = self.extract_information('span', {'data-qa': \"resume-block-specialization-category\"})\n",
    "        if area != \"No information\":\n",
    "            area = area.split(',')\n",
    "            area = area[0]\n",
    "        return area\n",
    "\n",
    "    def extract_wage(self):\n",
    "        wage = self.extract_information('span', {'data-qa': \"resume-block-salary\"})\n",
    "        int_pattern = r'\\d'\n",
    "        if wage != \"No information\":\n",
    "            wage_amount = re.findall(int_pattern, wage)\n",
    "            wage_str = ''.join(wage_amount)\n",
    "            wage = int(f'{wage_str}')\n",
    "            return wage\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "\n",
    "    def extract_languages(self):\n",
    "        languages = self.extract_information('p', {'data-qa': \"resume-block-language-item\"}, to_list=False)\n",
    "        if type(languages) == str:\n",
    "            languages = {\"Russian\": \"Native\"}\n",
    "        return languages\n",
    "\n",
    "    @property\n",
    "    def extract_gender(self):\n",
    "        gender = self.extract_information('span', {'data-qa': \"resume-personal-gender\"})\n",
    "        if gender == 'The man':\n",
    "            gender = 'Male'\n",
    "        elif gender == 'Woman':\n",
    "            gender = 'Female'\n",
    "        return gender\n",
    "\n",
    "\n",
    "    def extract_information(self, tag: str, attributes: dict, to_list=True):\n",
    "        finder = self.soup.find_all(tag, attributes)\n",
    "        if len(finder) == 1:\n",
    "            finder = finder[0].get_text()\n",
    "            return finder\n",
    "        elif len(finder) > 1:\n",
    "            if to_list:\n",
    "                listed_info = []\n",
    "                for part in finder:\n",
    "                    list_element = part.get_text()\n",
    "                    listed_info.append(list_element)\n",
    "                return listed_info\n",
    "            else:\n",
    "                dict_info = {}\n",
    "                for dict_part in finder:\n",
    "                    dict_element = dict_part.get_text()\n",
    "                    splitted_dict_element = dict_element.split()\n",
    "                    dict_info[splitted_dict_element[0]] = splitted_dict_element[2]\n",
    "                return dict_info\n",
    "        else:\n",
    "            return \"No information\"\n",
    "\n",
    "    def extract_nested_information(self, tag: str, attributes: dict, to_list=True):\n",
    "        finder = self.soup.find(tag, attributes)\n",
    "        if finder is None:\n",
    "            return \"No information\"\n",
    "        else:\n",
    "            finder = finder.find('span', {'class': \"resume-block__title-text resume-block__title-text_sub\"})\n",
    "            finder = finder.get_text()\n",
    "        return finder\n",
    "\n",
    "    def resume_dict_maker(self):\n",
    "        resume_dict = {'id': self.resume_id,\n",
    "                       'title': self.resume_title,\n",
    "                       'city': self.city,\n",
    "                       'age': self.age,\n",
    "                       'gender': self.gender,\n",
    "                       'area': self.area,\n",
    "                       'desired_wage': self.desired_wage,\n",
    "                       'work_experience': self.work_exp,\n",
    "                       'experience_description': self.experience_description,\n",
    "                       'education_level': self.education,\n",
    "                       'languages': self.language_prof,\n",
    "                       'skills': self.skills\n",
    "                       }\n",
    "        return resume_dict\n",
    "\n",
    "\n",
    "class ResumeGetter:\n",
    "#скачиваем резюме    \n",
    "    def __init__(self):\n",
    "        self.dir_path = \"./data/saved_resumes/\"\n",
    "        self.resume_dict_storage = []\n",
    "        with open('./data/id_list/id_list.txt', 'r') as f:\n",
    "            self.resume_storage = ['resume_page_' + line.strip() + '.html' for line in f]\n",
    "\n",
    "    def get_resume(self):\n",
    "        pbar = tqdm(total=len(self.resume_storage))\n",
    "\n",
    "        for resume in self.resume_storage:\n",
    "            with open(f'{self.dir_path}{resume}', \"r\") as resume_page:\n",
    "                resume_text = BeautifulSoup(resume_page, features=\"lxml\")\n",
    "                resume_id = str(re.search(r\"page_(.+)\\.html\", resume).group(1))\n",
    "                if self.check_if_exists(resume_id):\n",
    "                        print(f\"File {f'{resume_id}.json'} already exists. Skip\")\n",
    "                resume_getter = Resume(resume_text, resume_id)\n",
    "                dict_format = resume_getter.resume_dict_maker()\n",
    "                self.resume_dict_storage.append(dict_format)\n",
    "                self.get_json_resume(dict_format, resume_id)\n",
    "                pbar.update()  \n",
    "        return\n",
    "\n",
    "    def check_if_exists(self, res_id: str):\n",
    "        return os.path.exists(os.path.join('data/new_json_resumes/', f'{res_id}.json'))\n",
    "\n",
    "    def get_json_resume(self, resume_dict: dict, r_id: str):\n",
    "        with open(f'data/new_json_resumes/{r_id}.json', 'w+') as json_file:\n",
    "            json.dump(resume_dict, json_file, indent=4)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = ResumeGetter()\n",
    "    data.get_resume()\n",
    "    print(data.resume_dict_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a89692",
   "metadata": {},
   "source": [
    "Аккумулируем всю информацию в один json файл, а затем конвертируем его в csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for f in glob.glob(\"data/new_json_resumes/*.json\"):\n",
    "    with open(f, \"r\") as infile:\n",
    "        result.append(json.load(infile))\n",
    "\n",
    "with open(\"data/merged/merged_file.json\", \"w\") as outfile:\n",
    "     json.dump(result, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ab8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/merged/merged_file.json')\n",
    "df.to_csv('data/merged/merged_file.csv')\n",
    "print('Merged file saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b5988",
   "metadata": {},
   "source": [
    "На выходе имеем csv файл, содержащий в себе информацию из всех скачанных резюме."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
